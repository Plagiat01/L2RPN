{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grid2op.Agent import BaseAgent\n",
    "from grid2op.Agent import DoNothingAgent\n",
    "from grid2op import make\n",
    "from grid2op.PlotGrid import PlotMatplot\n",
    "from DenseNN import DenseNN\n",
    "from DQN import DQN\n",
    "import numpy as np\n",
    "from evaluate import evaluate\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"l2rpn_neurips_2020_track2_small\")\n",
    "plot_helper = PlotMatplot(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    The template to be used to create an agent: any controller of the power grid is expected to be a subclass of this\n",
    "    grid2op.Agent.BaseAgent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, curr_dir):\n",
    "      \"\"\"Initialize a new agent.\"\"\"\n",
    "      BaseAgent.__init__(self, action_space=env.action_space)\n",
    "\n",
    "      self.all_actions = [env.action_space({})]\n",
    "      actions = np.load(os.path.join(curr_dir, \"top1000_actions.npz\"), allow_pickle=True)[\"actions\"]\n",
    "      for action in actions:\n",
    "        self.all_actions.append(env.action_space.from_vect(action))\n",
    "      self.all_actions = np.asarray(self.all_actions)\n",
    "\n",
    "      self.dqn = DQN(self, (lambda : DenseNN(env.current_obs.to_vect().shape, self.all_actions.shape[0])))\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "      \"\"\"The action that your agent will choose depending on the observation, the reward, and whether the state is terminal\"\"\"\n",
    "      # do nothing for example (with the empty dictionary) :\n",
    "      \n",
    "      return self.dqn.select_action(observation)\n",
    "\n",
    "    def train(self, env, nb_episode):\n",
    "      self.dqn.replay_exp(env, nb_episode=nb_episode)\n",
    "    \n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    The template to be used to create an agent: any controller of the power grid is expected to be a subclass of this\n",
    "    grid2op.Agent.BaseAgent.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, curr_dir):\n",
    "      \"\"\"Initialize a new agent.\"\"\"\n",
    "      BaseAgent.__init__(self, action_space=env.action_space)\n",
    "\n",
    "      self.all_actions = [env.action_space({})]\n",
    "      actions = np.load(os.path.join(curr_dir, \"top1000_actions.npz\"), allow_pickle=True)[\"actions\"]\n",
    "      for action in actions:\n",
    "        self.all_actions.append(env.action_space.from_vect(action))\n",
    "      self.all_actions = np.asarray(self.all_actions)\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "      \"\"\"The action that your agent will choose depending on the observation, the reward, and whether the state is terminal\"\"\"\n",
    "      # do nothing for example (with the empty dictionary) :\n",
    "      \n",
    "      return np.random.choice(self.all_actions)\n",
    "\n",
    "\n",
    "    \n",
    "def make_agent(env, this_directory_path):\n",
    "  my_agent = DQNAgent(env, this_directory_path)\n",
    "  return my_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent = DQNAgent(env, \".\")\n",
    "nothing_agent = DoNothingAgent(env.action_space)\n",
    "dqn_agent.train(env, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"l2rpn_neurips_2020_track2_small\")\n",
    "evaluate(dqn_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make(\"l2rpn_neurips_2020_track2_small\")\n",
    "evaluate(nothing_agent, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env.name)\n",
    "obs = env.current_obs\n",
    "reward = env.current_reward\n",
    "done = False\n",
    "actions = [env.action_space({}), env.action_space({})]\n",
    "sum_r = 0\n",
    "for i in range(25):\n",
    "  action = dqn_agent.act()\n",
    "  obs, reward, done, info = env.step(action)\n",
    "  sum_r += reward\n",
    "  print(reward)\n",
    "  print(done)\n",
    "  if done:\n",
    "    break\n",
    "print(\"Sum\", sum_r)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
